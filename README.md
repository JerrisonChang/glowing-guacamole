# glowing-guacamole
This is a repository for ICSI531F20 Project for Team3

## Install
First make sure you have [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) and [python3](https://www.python.org/downloads/) installed
Run the following command in your terminal:
```bash
git --version
python3 --version
```

To install the repository run the follwing commands in your terminal:
```bash
git clone https://github.com/JerrisonChang/glowing-guacamole.git # Clones the repository to your local machine
cd glowing-guacamole # Moves you into the repository
pip3 install --upgrade pip # Upgrades pip to its latest version
pip3 install -r requirements.txt # Will install all of the required packages to run the code
```

## Dataset
Our [Dataset](https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=76&access_redir=1) is supplied by K.R Koedinger et. al.
The dataset is from a data repository for the EDM community: The PSLC DataShop.
This dataset has data from the area unit of the Geometry Cognitive Tutor course as it was used during the school year 1996-1997.  

## Bayesian Knowledge Tracing (BKT)
Michael Yudelson's [Standard Knowledge Tracing Package](https://iedms.github.io/standard-bkt/) is used for our experiment on our dataset.

The script in this section is developped using VS code, so the path defined in the scripts are with respect to the project root folder. As a result, when you run the code, make sure your current directory is at the top level (`glowing-guacamole`). Alternatively, open the `glowing-guacamole` folder with VS code and run the scripts described in this section.
### Getting and Compiling the BKT package
### Prerequisit for Different Platforms
- **MacOX:**<br/>
You would need:
    1. Xcode command line tools installed. 
    2. g++/gcc compiler with Open MP (no longer bundled with Mac OS X by default) could be downloaded from [hpc.sourceforge.net](http://hpc.sourceforge.net/)
    3. change the first 2 lines in `makefile` from `g++` to `g++-10`
- **Windows:**<br/>
    You might need to install `cygwin` and have `g++/gcc` compiler available and be sure to install `make` utility as well.
- **Linux:**<br/>
    The `g++/gcc` compiler and Open MP library should already be installed, you can proceed with the next step.

After running
```bash
git clone https://github.com/IEDMS/standard-bkt
```
navigate to the folder and issue the `make all` command, this will compile the code. Then copy `predicthmm` and `trainhmm` files that we will use. Copy them and paste them under the BKT folder.

### prepare_data.py
This script format the dataset into txt files that satisfied the requirements of the package. We split the each student by randomly pick 80% of the interactions as training set and the rest for test set. After running this code, you will get a set of training set and testing set for each student plus the whole processed clean data.

### generate_and_predict.py
This script will generate the BKT model for each student and store them in `BKT/model/` folder. It will also uses the generated model to prodice the outcome of the test data generated by `prepare_data.py`

### n_KC_calculation.py
This sctipt will evaluate the accuracy of the model if the interaction contains n knowledge components and output a `bkt_accuracies.csv` under `./BKT`
### generate_graph.py
This script will generate two graphs comparing the performance of BKT, RNN, and LSTM model: 
1. AUC of ROC curve graph.
2. accuracy with n hidden components.
## Deep Knowledge Tracing (DKT)
Performs Deep Knowledge Tracing on out data set using both simple RNN and LSTM networks.
deep knowldge tracing model was based off of [Lucas C. Casagrande's](https://github.com/lccasagrande) [implementation](https://github.com/lccasagrande/Deep-Knowledge-Tracing) of Deep Knoweledge Tracing.

### data_util.py
Configures the data from out dataset into a tensor of shape (student, question, knoweledge component).
Generates tensorflow dataset and splits data set into a traing set (80%) and a testing set (20%).

### deepkt.py
The Deep Knowledge Tracing Model

### metrics.py
The metrics to evaluate our models with

### Run
To run the deep knowldge tracing model, run the following command in your terminal:
```bash
cd DKT
python3 run_dkt.py
```
The code generates 15 diffrent Simple RNN and LSTM networks trained on data from the dataset.
Each model is trained on a diffrent number of knowldge components (KC) ranging from 1 to 15.

### Output
The output will be four files:
1. lstm_roc.csv - predicted values vs actual values from the testing set from the LSTM model trained on all KCs
2. rnn_roc.csv - predicted values vs actual values from the testing set from the RNN model trained on all KCs
3. lstm_accuracy.csv - accuracy of each LSTM model
4. rnn_accuracy.csv - accuracy of each RNN model

## Knowledge Tracing Machine (KTM) 
Performs knowldge tracing using factorization machines
Code is from [Jill-JÃªnn Vie's](https://github.com/jilljenn) [implementation](https://github.com/jilljenn/ktm) with slight modifications to perform knowldge tracing on our dataset.

### Install
To make the factorization machine libary run:
```bash
cd KTM
make libfm
```

### Perpare Data
To perpare the data for use by the factorization machine run the following:
```bash
cd data/geometry
python3 config_data.py # Outputs data.csv and needed.csv
cd ../..
```
data.csv has the following format:
```bash
user, item, skill, correct, wins, fails
```
needed.csv has the following format:
```bash
user_id, item_id, correct
```

### Run
To encode the data into sparse features run:
```bash
python3 encode.py --dataset geometry [--users] [--items] [--skills] [--wins] [--fails]  # To get the encodings (npz)
```
The options "--users", "--items", etc. denote the features to encode for the Q-matrix.
For example, if we wanted to encode users and items we would run:
```bash
python3 encode.py --dataset geometry --users --items  # Outputs Q-matrix file called X-ui.npz
```
And if we wanted to encode skills, wins, and fails we would run:
```bash
python3 encode.py --dataset geometry --skills --wins --fails  # Outputs Q-matrix file called X-swf.npz
```
Note the change in the Q-matrix file name to reflect the encoded features

To encode the time windows run:
```bash
python3 encode_tw.py --dataset geometry --tw # Will encode DAS3H sparse features into X.npz
```
This will also generate folds for the time windows which will be used in the MIRT and IRT models

To run the logistic regression model (d = 0), run the following:
```bash
python3 lr.py data/geometry/X-ui.npz
```

And to run the factorization machines (d > 0), run:
```bash
python3 fm.py --d 5 data/geometry/X-ui.npz # --d is the number of dimensions to run the factorization machine with
```
The above command will run the factorization machine on a Q-matrix with encodeings on users and items up to 5 dimensions

To run the MIRT model:
ENCODING, LR AND FM MUST HAVE BEEN RAN FOR USERS AND ITEMS FIRST. WILL NOT WORK WHEN RUN ON OTHER FEATURES
```bash
python3 omirt.py --d 0 data/geometry/needed.csv  # Will load LR: coef0.npy
python3 omirt.py --d 5 data/geometry/needed.csv  # Will load FM: w.npy and V.npy
```
The above two examples show the MIRT model will load diffrent parameters depending on the number of dimensions given
When d = 0 the MIRT model will load the logistics regression model
And when d > 0 it will load the factorization machine model

To run the IRT model, run:
```bash
python3 dmirt.py data/geometry/needed.csv
```
